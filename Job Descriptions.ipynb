{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Byron Dennis - Assignment 4\n",
    "Applying data mining algorithms to text!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "For this assignment I chose to use a dataset with job posting to see if it would be possible to predict job titles based on job requirements.  For the purposes of this analysis I selected a handful of job titles to focus on.\n",
    "\n",
    "'Accountant','Lawyer','Software Developer','Project Manager','Sales manager','Chief Accountant','Web Developer','Java Developer','Office Manager'\n",
    "\n",
    "I was curious to see if the model would have trouble differentiating chief accountants and accountants.  I also thought the model may have trouble with web developers, java developers, and software developers.\n",
    "\n",
    "https://www.kaggle.com/madhab/jobposts\n",
    "\n",
    "The dataset consists of 19,000 job postings that were posted through the Armenian human resource portal CareerCenter. The data was extracted from the Yahoo! mailing group https://groups.yahoo.com/neo/groups/careercenter-am. This was the only online human resource portal in the early 2000s. A job posting usually has some structure, although some fields of the posting are not necessarily filled out by the client (poster). The data was cleaned by removing posts that were not job related or had no structure. The data consists of job posts from 2004-2015 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "Write a short description of the results of these “baseline” models. Make sure your answer is no longer than four paragraphs, and should at minimum answer these questions:\n",
    "- What decisions did you make when creating your feature space? Why?\n",
    "- How do these classifiers address your question?\n",
    "- How did your models perform? What performance measure did you use?  Why? \n",
    "- Are you happy with the results?  Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base Line Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three baseline models used to predict job titles based on job requirements were a logistic regression, decision tree, and a K Nearest Neigbor model.  Logistic regression models estimate log odds for each observation that can be turned into probabilities to make a predictions.  Decision trees find the features that provide the most information to make a prediction and create an \"if then\" logic to classify data.  The KNN model identifies the specified number of observations (k) that are closest to the target and classifies based on those observations.\n",
    "\n",
    "The accuracy of the baseline models were a good starting point, but need additional work.  The logistic regression was an early stand out with 84% accuracy which was five percentage points greater than the next best model (Decision Tree).  The overall precision and recall were almost identical to the model accuracy, but as I suspected each classifier had trouble differentiating the various types of developers and accountants.  When the logistic regression predicted a job posting to be for a web developer it was only correct 66% of the time (precision).  The recall statistic for Java Developers was 62% which tells us that only 62% of total number of Java Developers were classified correctly.  The ineraction between precision and recall for Java Developers and Web Developers leads me to believe that Web Developers are being classified as Java Developers.\n",
    "\n",
    "My initial feature space was created by removing common english words that seemed to provided little or no value such as \"we\", \"our\" and \"he\".  There were also more specific job description words that were removed that provided no value to the analysis (e.g. llc, company, work, seeking, position, looking , candidate, incumbent and activities).  A additional option I applied to my initial feature space was to exclude words that appeared in 90% or more of the job descriptions.  When words appear in the majority of job descriptions they do not add any predictive value to the models and add additional noise when trying to figure out which words are truly important.  The final option I applied was to use counts rather than binary word indicators as I thought the frequency of certain words could be useful to the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Decision Tree (initial)</td>\n",
       "      <td>0.789474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression initial</td>\n",
       "      <td>0.839009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>K Nearest Neighbors initial (k=3)</td>\n",
       "      <td>0.718266</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Model  Accuracy\n",
       "0            Decision Tree (initial)  0.789474\n",
       "1        Logistic Regression initial  0.839009\n",
       "2  K Nearest Neighbors initial (k=3)  0.718266"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarydf1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Logistic Regression Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "        Accountant       0.81      0.87      0.84        87\n",
      "  Chief Accountant       0.76      0.71      0.74        49\n",
      "    Java Developer       0.89      0.62      0.73        26\n",
      "            Lawyer       1.00      1.00      1.00        45\n",
      "    Office Manager       1.00      0.90      0.95        20\n",
      "   Project Manager       1.00      0.89      0.94        35\n",
      "Software Developer       0.75      0.79      0.77        34\n",
      "     Web Developer       0.66      0.85      0.74        27\n",
      "\n",
      "       avg / total       0.85      0.84      0.84       323\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(clf2_expected, clf2_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "Write a short description of the improvement you were able to make in your prediction. Make sure your answer is no longer than four paragraphs, and should at minimum answer these questions:\n",
    "- What combination of classifiers and settings did you use and why?\n",
    "- Which model fit “best” and what metric did you use for the comparison? Why? \n",
    "- Are you happy with the results? Why or why not?  What could you do to improve on the “best” model’s performance?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 2\n",
    "After making several changes to my baseline models I was able to increase overall model accuracy from 84% to 87% using the logistic regression.  Precision and recall both increased from 85% to 87%.  I am still not satisfied with the final model because it still struggles to classify developers (especially java developers) and could use improvement when predicting chief accountants.  \n",
    "\n",
    "To improve the model fit I tried a variety of preprocessing options as well as different count vectorizer settings.  My first test was to find the optimal k for the KNN model to see if that would improve the KNN model.  It turns out that the optimal k reported was 1 which makes me uncomfortable because so little information is being taken into account to make a prediction.  The performance of the KNN model was only slightly improved by using the \"optimal\" k, so I decided to focus soley on improving the logistic regression model.\n",
    "\n",
    "The most impactful changes I made were changing my test/train ratio and using the binary setting for the count vectorizer.  The baseline models were developed using a 70/30 split for testing and training.  The dataset I was using only included 1,074 records.  Changing the test/train split to 80/20 gave the model more information which led to increased accuracy.  I believe using binary values from the count vectorizer improved the model because it allowed key words that were mentioned less often too have greater importance in the model. \n",
    "\n",
    "Other options I tried included removing additional stopwords, stemming (reduced model performance) and using tfidf weights.  One additional action that could be taken to improve the model would be to identify the words that the developer positions have in common and which words are exclusive to each position to see if there are additional changes that can be made to the stopwords that would improve the model.  I am concerned that some of the innaccuracy can be attributed to poor job description data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Decision Tree (initial)</td>\n",
       "      <td>0.789474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression initial</td>\n",
       "      <td>0.839009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>K Nearest Neighbors initial (k=3)</td>\n",
       "      <td>0.718266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>K Nearest Neighbors (k=1 optimal)</td>\n",
       "      <td>0.733746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Logistic Regression binary CV</td>\n",
       "      <td>0.851393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Decision Tree binary CV</td>\n",
       "      <td>0.743034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Logistic Regression 80/20, binary = True</td>\n",
       "      <td>0.869767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Logistic Regression 80/20, binary, Stem</td>\n",
       "      <td>0.832558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Logistic Regression 80/20, Stem, stopwords</td>\n",
       "      <td>0.818605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Logistic Regression 80/20, tfidf</td>\n",
       "      <td>0.818605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Logistic Regression 80/20, dict, new stopwords, binary</td>\n",
       "      <td>0.865116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     Model  Accuracy\n",
       "0                                  Decision Tree (initial)  0.789474\n",
       "1                              Logistic Regression initial  0.839009\n",
       "2                        K Nearest Neighbors initial (k=3)  0.718266\n",
       "3                        K Nearest Neighbors (k=1 optimal)  0.733746\n",
       "4                            Logistic Regression binary CV  0.851393\n",
       "5                                  Decision Tree binary CV  0.743034\n",
       "6                 Logistic Regression 80/20, binary = True  0.869767\n",
       "7                  Logistic Regression 80/20, binary, Stem  0.832558\n",
       "8               Logistic Regression 80/20, Stem, stopwords  0.818605\n",
       "9                         Logistic Regression 80/20, tfidf  0.818605\n",
       "10  Logistic Regression 80/20, dict, new stopwords, binary  0.865116"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarydf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Logistic Regression Model Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "        Accountant       0.87      0.91      0.89        57\n",
      "  Chief Accountant       0.82      0.79      0.81        34\n",
      "    Java Developer       0.83      0.59      0.69        17\n",
      "            Lawyer       1.00      1.00      1.00        28\n",
      "    Office Manager       1.00      0.93      0.97        15\n",
      "   Project Manager       1.00      0.90      0.95        21\n",
      "Software Developer       0.74      0.92      0.82        25\n",
      "     Web Developer       0.78      0.78      0.78        18\n",
      "\n",
      "       avg / total       0.87      0.87      0.87       215\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(clf5_expected, clf5_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "Write a short description of the outcome of your clustering exercise. Make sure your answer is no longer than four paragraphs, and should at minimum answer these questions:\n",
    "- How many clusters did you “find”? Why did you select that number? \n",
    "- Can you easily describe your clusters with a text label?  If so, what are the labels and how do they help?  If not, why not? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose to look for 8 clusters because that is the number of distinct job titles that were included in my data set.  After running the clustering model I created a summary with the count of each job description within each cluster.  Some clusters were clearly separated.  For example, one cluster contained almost all of the lawyers and another cluster contained all of the developer positions (web, software and java developers).  However, 5 separate clusters were created that included accountants and  chief accountants.  The analysis also grouped project managers and office managers together.\n",
    "\n",
    "I cannot easily describe all of the clusters based on the groupings of job descriptions because multiple clusters were created for the same job descriptions.  Even though the clusters are not clearly defined at this point two things could come out of this analysis.  I could try to change the number of clusters from 8 to a lesser number to see if the jobs would be classified in a more meaningful way (i.e. one cluster for accounts, one cluster for developers, etc.).  I could also leave the clusters in their current state and further evaluate the difference is in job descriptions to see if the job descriptions need more clarification or title changes.  Perhaps the cluster results are indicating vague job descriptions or poorly classified job requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Housekeeping\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from __future__ import division\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "\n",
    "pd.set_option('display.max_colwidth', 18000)\n",
    "\n",
    "pathname = \"C:/Users/byron/OneDrive/Documents/Text Mining/Job_Posts/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19001, 24)\n",
      "['jobpost', 'date', 'Title', 'Company', 'AnnouncementCode', 'Term', 'Eligibility', 'Audience', 'StartDate', 'Duration', 'Location', 'JobDescription', 'JobRequirment', 'RequiredQual', 'Salary', 'ApplicationP', 'OpeningDate', 'Deadline', 'Notes', 'AboutC', 'Attach', 'Year', 'Month', 'IT']\n"
     ]
    }
   ],
   "source": [
    "# Get data\n",
    "\n",
    "df = pd.read_csv(pathname + \"data job posts.csv\")\n",
    "\n",
    "print(df.shape)\n",
    "print(list(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1074, 24)\n"
     ]
    }
   ],
   "source": [
    "# Remove NAs\n",
    "jobdf = df[df.JobRequirment.notnull()]\n",
    "\n",
    "# Choose specific job titles to classify\n",
    "\n",
    "value_list = ['Accountant','Lawyer','Software Developer','Project Manager','Sales manager','Chief Accountant','Web Developer','Java Developer','Office Manager']\n",
    "jobdf = jobdf[jobdf.Title.isin(value_list)]\n",
    "\n",
    "print(jobdf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#jobdf.JobRequirment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1074, 3493)\n"
     ]
    }
   ],
   "source": [
    "# Initial Vectorizer Review\n",
    "prelim = CountVectorizer(stop_words='english', max_df = .9)\n",
    "prelim_dm = prelim.fit_transform(jobdf['JobRequirment'])\n",
    "\n",
    "print(prelim_dm.shape)\n",
    "\n",
    "names = prelim.get_feature_names()\n",
    "\n",
    "count = np.sum(prelim_dm.toarray(), axis = 0).tolist()\n",
    "\n",
    "count_df = pd.DataFrame(count, index = names, columns = ['count'])\n",
    "\n",
    "#count_df.sort_values(['count'], ascending = False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Custom Stop Words\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk_stopwords = stopwords.words(\"english\")\n",
    "\n",
    "my_stopwords = nltk_stopwords + ['llc','company','work','seeking','position','looking','candidate'\n",
    "                                 ,'incumbent','activities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1074, 3550)\n"
     ]
    }
   ],
   "source": [
    "# instantiate vectorizer(s)\n",
    "cv = CountVectorizer(stop_words=my_stopwords,\n",
    "                     max_df=.9)\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words=my_stopwords,\n",
    "                     max_df=.9)\n",
    "\n",
    "\n",
    "# fit text\n",
    "cv_dm = cv.fit_transform(jobdf['JobRequirment'])\n",
    "tfidf_dm = tfidf.fit_transform(jobdf['JobRequirment'])\n",
    "\n",
    "print(cv_dm.shape)\n",
    "\n",
    "names = cv.get_feature_names()\n",
    "\n",
    "count = np.sum(cv_dm.toarray(), axis = 0).tolist()\n",
    "\n",
    "count_df = pd.DataFrame(count, index = names, columns = ['count'])\n",
    "\n",
    "#count_df.sort_values(['count'], ascending = False).head(20)\n",
    "\n",
    "# print(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "(751, 3560)\n",
      "(323, 3560)\n",
      "(751,)\n",
      "(323,)\n"
     ]
    }
   ],
   "source": [
    "# train test split\n",
    "# data are X, labels are y\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = cv_dm.toarray() \n",
    "print(type(X))\n",
    "\n",
    "y = jobdf['Title'].values #this is an array of labels\n",
    "print(type(y))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) #random_state is set seed\n",
    "\n",
    "# function creates 4 output structures - order matters\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=42, splitter='best')\n",
      "0.789473684211\n",
      "accuracy: 0.789473684211\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "        Accountant       0.77      0.76      0.76        87\n",
      "  Chief Accountant       0.60      0.73      0.66        49\n",
      "    Java Developer       0.94      0.58      0.71        26\n",
      "            Lawyer       1.00      0.96      0.98        45\n",
      "    Office Manager       0.94      0.80      0.86        20\n",
      "   Project Manager       1.00      0.77      0.87        35\n",
      "Software Developer       0.81      0.76      0.79        34\n",
      "     Web Developer       0.62      0.96      0.75        27\n",
      "\n",
      "       avg / total       0.82      0.79      0.79       323\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf1_nm = \"Decision Tree (initial)\"\n",
    "\n",
    "model = DecisionTreeClassifier(random_state = 42)\n",
    "print(model)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "clf1_expected = y_test\n",
    "clf1_predicted = model.predict(X_test)\n",
    "\n",
    "print(model.score(X_test, y_test))\n",
    "\n",
    "# summarize the fit of the model\n",
    "print(\"accuracy: \" + str(metrics.accuracy_score(clf1_expected, clf1_predicted)))\n",
    "print(metrics.classification_report(clf1_expected, clf1_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=42, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "0.839009287926\n",
      "accuracy: 0.839009287926\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "        Accountant       0.81      0.87      0.84        87\n",
      "  Chief Accountant       0.76      0.71      0.74        49\n",
      "    Java Developer       0.89      0.62      0.73        26\n",
      "            Lawyer       1.00      1.00      1.00        45\n",
      "    Office Manager       1.00      0.90      0.95        20\n",
      "   Project Manager       1.00      0.89      0.94        35\n",
      "Software Developer       0.75      0.79      0.77        34\n",
      "     Web Developer       0.66      0.85      0.74        27\n",
      "\n",
      "       avg / total       0.85      0.84      0.84       323\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf2_nm = \"Logistic Regression initial\"\n",
    "\n",
    "# fit a logistic regression model to the data\n",
    "model = LogisticRegression(random_state = 42)\n",
    "print(model)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "clf2_expected = y_test\n",
    "clf2_predicted = model.predict(X_test)\n",
    "\n",
    "print(model.score(X_test, y_test))\n",
    "\n",
    "# summarize the fit of the model\n",
    "print(\"accuracy: \" + str(metrics.accuracy_score(clf2_expected, clf2_predicted)))\n",
    "print(metrics.classification_report(clf2_expected, clf2_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.71826625387\n",
      "accuracy: 0.71826625387\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "        Accountant       0.71      0.89      0.79        87\n",
      "  Chief Accountant       0.69      0.69      0.69        49\n",
      "    Java Developer       0.56      0.58      0.57        26\n",
      "            Lawyer       1.00      0.84      0.92        45\n",
      "    Office Manager       1.00      0.30      0.46        20\n",
      "   Project Manager       1.00      0.46      0.63        35\n",
      "Software Developer       0.73      0.71      0.72        34\n",
      "     Web Developer       0.49      0.81      0.61        27\n",
      "\n",
      "       avg / total       0.77      0.72      0.71       323\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn1_nm = \"K Nearest Neighbors initial (k=3)\"\n",
    "\n",
    "# fit KNN\n",
    "model = KNeighborsClassifier(n_neighbors=3)\n",
    "model.fit(X_train, y_train) \n",
    "\n",
    "# make predictions\n",
    "knn1_expected = y_test\n",
    "knn1_predicted = model.predict(X_test)\n",
    "\n",
    "print(model.score(X_test, y_test))\n",
    "\n",
    "# summarize the fit of the model\n",
    "print(\"accuracy: \" + str(metrics.accuracy_score(knn1_expected, knn1_predicted)))\n",
    "print(metrics.classification_report(knn1_expected, knn1_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Decision Tree (initial)</td>\n",
       "      <td>0.789474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression initial</td>\n",
       "      <td>0.839009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>K Nearest Neighbors initial (k=3)</td>\n",
       "      <td>0.718266</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Model  Accuracy\n",
       "0            Decision Tree (initial)  0.789474\n",
       "1        Logistic Regression initial  0.839009\n",
       "2  K Nearest Neighbors initial (k=3)  0.718266"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf1_accuracy = metrics.accuracy_score(clf1_expected, clf1_predicted)\n",
    "clf2_accuracy = metrics.accuracy_score(clf2_expected, clf2_predicted)\n",
    "knn1_accuracy = metrics.accuracy_score(knn1_expected, knn1_predicted)\n",
    "\n",
    "model_name = [clf1_nm, clf2_nm, knn1_nm]\n",
    "model_accuracy = [clf1_accuracy, clf2_accuracy, knn1_accuracy]\n",
    "\n",
    "summarydf1 = pd.DataFrame(model_name, columns=['Model'])\n",
    "summarydf1['Accuracy']=(model_accuracy)\n",
    "summarydf1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - Improve Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy could be better.  Developer jobs seem to be a problem.  First let's try to optimize KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# KNN Cross Validation\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# creating odd list of K for KNN\n",
    "myList = list(range(1,35))\n",
    "\n",
    "# subsetting just the odd ones\n",
    "neighbors = list(filter(lambda x: x % 2 != 0, myList))\n",
    "\n",
    "# empty list that will hold cv scores\n",
    "cv_scores = []\n",
    "\n",
    "#takes some time, get a soda...\n",
    "# perform 10-fold cross validation\n",
    "for k in neighbors:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X_train, y_train, cv=10, scoring='accuracy')\n",
    "    cv_scores.append(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal number of neighbors is 1\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x16a79d6d4a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# changing to misclassification error\n",
    "MSE = [1 - x for x in cv_scores]\n",
    "\n",
    "# determining best k\n",
    "optimal_k = neighbors[MSE.index(min(MSE))]\n",
    "print(\"The optimal number of neighbors is %d\" % optimal_k)\n",
    "\n",
    "# plot misclassification error vs k\n",
    "plt.plot(neighbors, MSE)\n",
    "plt.xlabel('Number of Neighbors K')\n",
    "plt.ylabel('Misclassification Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.733746130031\n",
      "accuracy: 0.733746130031\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "        Accountant       0.72      0.82      0.76        87\n",
      "  Chief Accountant       0.67      0.71      0.69        49\n",
      "    Java Developer       0.64      0.62      0.63        26\n",
      "            Lawyer       1.00      0.87      0.93        45\n",
      "    Office Manager       1.00      0.35      0.52        20\n",
      "   Project Manager       0.95      0.54      0.69        35\n",
      "Software Developer       0.62      0.85      0.72        34\n",
      "     Web Developer       0.62      0.78      0.69        27\n",
      "\n",
      "       avg / total       0.77      0.73      0.73       323\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# New Models\n",
    "\n",
    "knn2_nm = \"K Nearest Neighbors (k=1 optimal)\"\n",
    "\n",
    "# fit KNN\n",
    "model = KNeighborsClassifier(n_neighbors=1)\n",
    "model.fit(X_train, y_train) \n",
    "\n",
    "# make predictions\n",
    "knn2_expected = y_test\n",
    "knn2_predicted = model.predict(X_test)\n",
    "\n",
    "print(model.score(X_test, y_test))\n",
    "\n",
    "# summarize the fit of the model\n",
    "print(\"accuracy: \" + str(metrics.accuracy_score(knn2_expected, knn2_predicted)))\n",
    "print(metrics.classification_report(knn2_expected, knn2_predicted))\n",
    "\n",
    "knn2_accuracy = metrics.accuracy_score(knn2_expected, knn2_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN is better, but not good enough.   Let's try changing our vectorizer settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1074, 3560)\n"
     ]
    }
   ],
   "source": [
    "# What happens if we use binary instead of counts?\n",
    "\n",
    "# instantiate vectorizer(s)\n",
    "cv2 = CountVectorizer(stop_words=my_stopwords,\n",
    "                     max_df=.9, binary = True)\n",
    "\n",
    "tfidf2 = TfidfVectorizer(stop_words=my_stopwords,\n",
    "                     max_df=.9, binary = True)\n",
    "\n",
    "\n",
    "# fit text\n",
    "cv_dm2 = cv2.fit_transform(jobdf['JobRequirment'])\n",
    "tfidf_dm2 = tfidf2.fit_transform(jobdf['JobRequirment'])\n",
    "\n",
    "print(cv_dm2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "(751, 3560)\n",
      "(323, 3560)\n",
      "(751,)\n",
      "(323,)\n"
     ]
    }
   ],
   "source": [
    "# Test / Train Split\n",
    "\n",
    "X2 = cv_dm2.toarray() \n",
    "print(type(X))\n",
    "\n",
    "y2 = jobdf['Title'].values #this is an array of labels\n",
    "print(type(y))\n",
    "\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=0.3, random_state=42) #random_state is set seed\n",
    "\n",
    "# function creates 4 output structures - order matters\n",
    "print(X_train2.shape)\n",
    "print(X_test2.shape)\n",
    "print(y_train2.shape)\n",
    "print(y_test2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=42, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "0.851393188854\n",
      "accuracy: 0.851393188854\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "        Accountant       0.88      0.85      0.87        87\n",
      "  Chief Accountant       0.74      0.82      0.78        49\n",
      "    Java Developer       0.80      0.62      0.70        26\n",
      "            Lawyer       1.00      1.00      1.00        45\n",
      "    Office Manager       1.00      0.95      0.97        20\n",
      "   Project Manager       1.00      0.89      0.94        35\n",
      "Software Developer       0.75      0.79      0.77        34\n",
      "     Web Developer       0.68      0.85      0.75        27\n",
      "\n",
      "       avg / total       0.86      0.85      0.85       323\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf3_nm = \"Logistic Regression binary CV\"\n",
    "\n",
    "# fit a logistic regression model to the data\n",
    "model = LogisticRegression(random_state = 42)\n",
    "print(model)\n",
    "model.fit(X_train2, y_train2)\n",
    "\n",
    "# make predictions\n",
    "clf3_expected = y_test2\n",
    "clf3_predicted = model.predict(X_test2)\n",
    "\n",
    "print(model.score(X_test2, y_test2))\n",
    "\n",
    "# summarize the fit of the model\n",
    "print(\"accuracy: \" + str(metrics.accuracy_score(clf3_expected, clf3_predicted)))\n",
    "print(metrics.classification_report(clf3_expected, clf3_predicted))\n",
    "\n",
    "clf3_accuracy = metrics.accuracy_score(clf3_expected, clf3_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=42, splitter='best')\n",
      "0.743034055728\n",
      "accuracy: 0.743034055728\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "        Accountant       0.74      0.72      0.73        87\n",
      "  Chief Accountant       0.60      0.73      0.66        49\n",
      "    Java Developer       0.72      0.50      0.59        26\n",
      "            Lawyer       0.95      0.93      0.94        45\n",
      "    Office Manager       0.81      0.65      0.72        20\n",
      "   Project Manager       0.89      0.71      0.79        35\n",
      "Software Developer       0.83      0.74      0.78        34\n",
      "     Web Developer       0.55      0.85      0.67        27\n",
      "\n",
      "       avg / total       0.76      0.74      0.75       323\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf4_nm = \"Decision Tree binary CV\"\n",
    "\n",
    "model = DecisionTreeClassifier(random_state = 42)\n",
    "print(model)\n",
    "model.fit(X_train2, y_train2)\n",
    "\n",
    "# make predictions\n",
    "clf4_expected = y_test2\n",
    "clf4_predicted = model.predict(X_test2)\n",
    "\n",
    "print(model.score(X_test2, y_test2))\n",
    "\n",
    "# summarize the fit of the model\n",
    "print(\"accuracy: \" + str(metrics.accuracy_score(clf4_expected, clf4_predicted)))\n",
    "print(metrics.classification_report(clf4_expected, clf4_predicted))\n",
    "\n",
    "clf4_accuracy = metrics.accuracy_score(clf4_expected, clf4_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check results from using binary vector settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Decision Tree (initial)</td>\n",
       "      <td>0.789474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression initial</td>\n",
       "      <td>0.839009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>K Nearest Neighbors initial (k=3)</td>\n",
       "      <td>0.718266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>K Nearest Neighbors (k=1 optimal)</td>\n",
       "      <td>0.733746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Logistic Regression binary CV</td>\n",
       "      <td>0.851393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Decision Tree binary CV</td>\n",
       "      <td>0.743034</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Model  Accuracy\n",
       "0            Decision Tree (initial)  0.789474\n",
       "1        Logistic Regression initial  0.839009\n",
       "2  K Nearest Neighbors initial (k=3)  0.718266\n",
       "3  K Nearest Neighbors (k=1 optimal)  0.733746\n",
       "4      Logistic Regression binary CV  0.851393\n",
       "5            Decision Tree binary CV  0.743034"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = [clf1_nm, clf2_nm, knn1_nm, knn2_nm, clf3_nm, clf4_nm]\n",
    "model_accuracy = [clf1_accuracy, clf2_accuracy, knn1_accuracy, knn2_accuracy, clf3_accuracy, clf4_accuracy]\n",
    "\n",
    "summarydf = pd.DataFrame(model_name, columns=['Model'])\n",
    "summarydf['Accuracy']=(model_accuracy)\n",
    "summarydf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How about a bigger training set since the data set is small?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "(859, 3560)\n",
      "(215, 3560)\n",
      "(859,)\n",
      "(215,)\n"
     ]
    }
   ],
   "source": [
    "# Test / Train Split\n",
    "\n",
    "X3 = cv_dm2.toarray() \n",
    "print(type(X))\n",
    "\n",
    "y3 = jobdf['Title'].values #this is an array of labels\n",
    "print(type(y))\n",
    "\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(X3, y3, test_size=0.2, random_state=42) #random_state is set seed\n",
    "\n",
    "# function creates 4 output structures - order matters\n",
    "print(X_train3.shape)\n",
    "print(X_test3.shape)\n",
    "print(y_train3.shape)\n",
    "print(y_test3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=42, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "0.86976744186\n",
      "accuracy: 0.86976744186\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "        Accountant       0.87      0.91      0.89        57\n",
      "  Chief Accountant       0.82      0.79      0.81        34\n",
      "    Java Developer       0.83      0.59      0.69        17\n",
      "            Lawyer       1.00      1.00      1.00        28\n",
      "    Office Manager       1.00      0.93      0.97        15\n",
      "   Project Manager       1.00      0.90      0.95        21\n",
      "Software Developer       0.74      0.92      0.82        25\n",
      "     Web Developer       0.78      0.78      0.78        18\n",
      "\n",
      "       avg / total       0.87      0.87      0.87       215\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf5_nm = \"Logistic Regression 80/20, binary = True\"\n",
    "\n",
    "# fit a logistic regression model to the data\n",
    "model = LogisticRegression(random_state = 42)\n",
    "print(model)\n",
    "model.fit(X_train3, y_train3)\n",
    "\n",
    "# make predictions\n",
    "clf5_expected = y_test3\n",
    "clf5_predicted = model.predict(X_test3)\n",
    "\n",
    "print(model.score(X_test3, y_test3))\n",
    "\n",
    "# summarize the fit of the model\n",
    "print(\"accuracy: \" + str(metrics.accuracy_score(clf5_expected, clf5_predicted)))\n",
    "print(metrics.classification_report(clf5_expected, clf5_predicted))\n",
    "\n",
    "clf5_accuracy = metrics.accuracy_score(clf5_expected, clf5_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about stemming?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# What happens if we use stemming?\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "jobdf['pstem'] = jobdf[\"JobRequirment\"].apply(lambda x: [ps.stem(y) for y in x.split()])\n",
    "jobdf['pstem']= [\" \".join(token) for token in jobdf['pstem']]\n",
    "#jobdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1074, 3220)\n"
     ]
    }
   ],
   "source": [
    "# instantiate vectorizer(s)\n",
    "\n",
    "# fit text\n",
    "cv_dm = cv2.fit_transform(jobdf['pstem'])\n",
    "tfidf_dm = tfidf2.fit_transform(jobdf['pstem'])\n",
    "\n",
    "print(cv_dm.shape)\n",
    "\n",
    "names = cv2.get_feature_names()\n",
    "\n",
    "count = np.sum(cv_dm.toarray(), axis = 0).tolist()\n",
    "\n",
    "count_df = pd.DataFrame(count, index = names, columns = ['count'])\n",
    "\n",
    "#count_df.sort_values(['count'], ascending = False).head(20)\n",
    "\n",
    "\n",
    "# print(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "(859, 3220)\n",
      "(215, 3220)\n",
      "(859,)\n",
      "(215,)\n"
     ]
    }
   ],
   "source": [
    "# Test / Train Split\n",
    "\n",
    "X = cv_dm.toarray() \n",
    "print(type(X))\n",
    "\n",
    "y = jobdf['Title'].values #this is an array of labels\n",
    "print(type(y))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) #random_state is set seed\n",
    "\n",
    "# function creates 4 output structures - order matters\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=42, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "0.832558139535\n",
      "accuracy: 0.832558139535\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "        Accountant       0.81      0.84      0.83        57\n",
      "  Chief Accountant       0.74      0.76      0.75        34\n",
      "    Java Developer       0.82      0.53      0.64        17\n",
      "            Lawyer       1.00      0.96      0.98        28\n",
      "    Office Manager       1.00      0.87      0.93        15\n",
      "   Project Manager       0.95      0.95      0.95        21\n",
      "Software Developer       0.73      0.88      0.80        25\n",
      "     Web Developer       0.74      0.78      0.76        18\n",
      "\n",
      "       avg / total       0.84      0.83      0.83       215\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf6_nm = \"Logistic Regression 80/20, binary, Stem\"\n",
    "\n",
    "# fit a logistic regression model to the data\n",
    "model = LogisticRegression(random_state = 42)\n",
    "print(model)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "clf6_expected = y_test\n",
    "clf6_predicted = model.predict(X_test)\n",
    "\n",
    "print(model.score(X_test, y_test))\n",
    "\n",
    "# summarize the fit of the model\n",
    "print(\"accuracy: \" + str(metrics.accuracy_score(clf6_expected, clf6_predicted)))\n",
    "print(metrics.classification_report(clf6_expected, clf6_predicted))\n",
    "\n",
    "clf6_accuracy = metrics.accuracy_score(clf6_expected, clf6_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lost accuracy.  I think this is because stopwords were not removed prior to stemming.  Try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# What happens if we remove stopwords before using stemming?\n",
    "from nltk.corpus import words\n",
    "\n",
    "#lowercase\n",
    "jobdf = jobdf.apply(lambda x: x.astype(str).str.lower())\n",
    "\n",
    "jobdf['cleantext']=jobdf['JobRequirment']\n",
    "jobdf['cleantext']=jobdf.cleantext.apply(lambda x: ' '.join([word for word in x.split() if word not in my_stopwords]))\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "jobdf['pstemclean'] = jobdf[\"cleantext\"].apply(lambda x: [ps.stem(n) for n in x.split()])\n",
    "jobdf['pstemclean']= [\" \".join(token) for token in jobdf['pstemclean']]\n",
    "#jobdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#jobdf['pstemclean'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1074, 3210)\n"
     ]
    }
   ],
   "source": [
    "# instantiate vectorizer(s)\n",
    "\n",
    "# fit text\n",
    "cv_dm = cv2.fit_transform(jobdf['pstemclean'])\n",
    "tfidf_dm = tfidf2.fit_transform(jobdf['pstemclean'])\n",
    "\n",
    "print(cv_dm.shape)\n",
    "\n",
    "names = cv2.get_feature_names()\n",
    "\n",
    "count = np.sum(cv_dm.toarray(), axis = 0).tolist()\n",
    "\n",
    "count_df = pd.DataFrame(count, index = names, columns = ['count'])\n",
    "\n",
    "#count_df.sort_values(['count'], ascending = False).head(20)\n",
    "\n",
    "\n",
    "# print(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "(859, 3210)\n",
      "(215, 3210)\n",
      "(859,)\n",
      "(215,)\n"
     ]
    }
   ],
   "source": [
    "# Test / Train Split\n",
    "\n",
    "X = cv_dm.toarray() \n",
    "print(type(X))\n",
    "\n",
    "y = jobdf['Title'].values #this is an array of labels\n",
    "print(type(y))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) #random_state is set seed\n",
    "\n",
    "# function creates 4 output structures - order matters\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=42, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "0.818604651163\n",
      "accuracy: 0.818604651163\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "        accountant       0.81      0.81      0.81        57\n",
      "  chief accountant       0.70      0.76      0.73        34\n",
      "    java developer       0.80      0.47      0.59        17\n",
      "            lawyer       1.00      0.96      0.98        28\n",
      "    office manager       1.00      0.87      0.93        15\n",
      "   project manager       0.95      0.95      0.95        21\n",
      "software developer       0.71      0.88      0.79        25\n",
      "     web developer       0.74      0.78      0.76        18\n",
      "\n",
      "       avg / total       0.83      0.82      0.82       215\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf7_nm = \"Logistic Regression 80/20, Stem, stopwords\"\n",
    "\n",
    "# fit a logistic regression model to the data\n",
    "model = LogisticRegression(random_state = 42)\n",
    "print(model)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "clf7_expected = y_test\n",
    "clf7_predicted = model.predict(X_test)\n",
    "\n",
    "print(model.score(X_test, y_test))\n",
    "\n",
    "# summarize the fit of the model\n",
    "print(\"accuracy: \" + str(metrics.accuracy_score(clf7_expected, clf7_predicted)))\n",
    "print(metrics.classification_report(clf7_expected, clf7_predicted))\n",
    "\n",
    "clf7_accuracy = metrics.accuracy_score(clf7_expected, clf7_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try different vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1074, 3560)\n"
     ]
    }
   ],
   "source": [
    "# instantiate vectorizer(s)\n",
    "cv = CountVectorizer(stop_words=my_stopwords,\n",
    "                     max_df=.9,\n",
    "                    binary = True)\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words=my_stopwords,\n",
    "                     max_df=.9,\n",
    "                       binary = True)\n",
    "\n",
    "\n",
    "# fit text\n",
    "cv_dm = cv.fit_transform(jobdf['JobRequirment'])\n",
    "tfidf_dm = tfidf.fit_transform(jobdf['JobRequirment'])\n",
    "\n",
    "print(cv_dm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "(859, 3560)\n",
      "(215, 3560)\n",
      "(859,)\n",
      "(215,)\n"
     ]
    }
   ],
   "source": [
    "# Test / Train Split\n",
    "\n",
    "X = tfidf_dm.toarray() \n",
    "print(type(X))\n",
    "\n",
    "y = jobdf['Title'].values #this is an array of labels\n",
    "print(type(y))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) #random_state is set seed\n",
    "\n",
    "# function creates 4 output structures - order matters\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=42, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "0.818604651163\n",
      "accuracy: 0.818604651163\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "        accountant       0.74      0.95      0.83        57\n",
      "  chief accountant       0.88      0.65      0.75        34\n",
      "    java developer       0.70      0.41      0.52        17\n",
      "            lawyer       1.00      1.00      1.00        28\n",
      "    office manager       1.00      0.67      0.80        15\n",
      "   project manager       0.95      0.95      0.95        21\n",
      "software developer       0.71      0.80      0.75        25\n",
      "     web developer       0.75      0.83      0.79        18\n",
      "\n",
      "       avg / total       0.83      0.82      0.81       215\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf8_nm = \"Logistic Regression 80/20, tfidf\"\n",
    "\n",
    "# fit a logistic regression model to the data\n",
    "model = LogisticRegression(random_state = 42)\n",
    "print(model)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "clf8_expected = y_test\n",
    "clf8_predicted = model.predict(X_test)\n",
    "\n",
    "print(model.score(X_test, y_test))\n",
    "\n",
    "# summarize the fit of the model\n",
    "print(\"accuracy: \" + str(metrics.accuracy_score(clf8_expected, clf8_predicted)))\n",
    "print(metrics.classification_report(clf8_expected, clf8_predicted))\n",
    "\n",
    "clf8_accuracy = metrics.accuracy_score(clf8_expected, clf8_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perhaps some custom replacement will help with accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#print(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#additional stop words that may improve the model\n",
    "add_stop = ['00', '09', '10', '14', '18', '20', '2000', '2006', '2007', '2008', '2009', '2010', '220', '228', '24', '3g', '3rd','aghekyan','armenia', 'armenian', 'armenias',\n",
    " 'ffective','armentel','arzni','boghossian', 'bono', 'asdg','candidate', 'candidates', 'canditate']\n",
    "\n",
    "my_stopwords = my_stopwords + add_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "job_dict = {'coordinates':'coordinate','coordinating':'coordinate','entries':'entry','challenges':'challenge',\n",
    "            'challenging':'challenge','compilation':'compile', 'compiling':'compile', 'completed':'complete', 'completely':'complete',\n",
    "            'completeness':'complete', 'completing':'complete', 'completion':'complete','cooperation':'cooperate','generally':'general',\n",
    "           'bookeeping':'book', 'bookkeeper':'book', 'bookkeeping':'book', 'books':'book','complexity':'complex','compliance':'compliant',\n",
    "            'complies':'compliant','comply':'compliant','complying':'compliant','reconciles':'reconcile', 'reconciliation':'reconcile',\n",
    "            'reconciling':'reconcile','\\r\\n':' ','-':' ','\\r':' ','\\n':' '}\n",
    "\n",
    "\n",
    "def multiple_replace(dict, text): \n",
    "  \"\"\" Replace in 'text' all occurences of any key in the given\n",
    "  dictionary by its corresponding value.  Returns the new tring.\"\"\" \n",
    "  text = str(text).lower()\n",
    "\n",
    "  # Create a regular expression  from the dictionary keys\n",
    "  regex = re.compile(\"(%s)\" % \"|\".join(map(re.escape, dict.keys())))\n",
    "\n",
    "  # For each match, look-up corresponding value in dictionary\n",
    "  return regex.sub(lambda mo: dict[mo.string[mo.start():mo.end()]], text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jobdf['cleantext'] = jobdf.JobRequirment.apply(lambda x: multiple_replace(job_dict, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove punctuation\n",
    "\n",
    "#import string\n",
    "\n",
    "#jobdf['cleantext'] = \"\".join(l for l in jobdf['cleantext'] if l not in string.punctuation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#jobdf['cleantext'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1074, 3529)\n"
     ]
    }
   ],
   "source": [
    "# instantiate vectorizer(s)\n",
    "cv = CountVectorizer(stop_words=my_stopwords,\n",
    "                     max_df=.9,\n",
    "                    binary = True)\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words=my_stopwords,\n",
    "                     max_df=.9,\n",
    "                       binary = True)\n",
    "\n",
    "# fit text\n",
    "cv_dm = cv.fit_transform(jobdf['cleantext'])\n",
    "tfidf_dm = cv.fit_transform(jobdf['cleantext'])\n",
    "\n",
    "job_dm = tfidf_dm.toarray()\n",
    "\n",
    "print(cv_dm.shape)\n",
    "\n",
    "#names = cv.get_feature_names()\n",
    "\n",
    "#count = np.sum(cv_dm.toarray(), axis = 0).tolist()\n",
    "\n",
    "#count_df = pd.DataFrame(count, index = names, columns = ['count'])\n",
    "\n",
    "#count_df.sort_values(['count'], ascending = False).head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "(859, 3529)\n",
      "(215, 3529)\n",
      "(859,)\n",
      "(215,)\n"
     ]
    }
   ],
   "source": [
    "# Test / Train Split\n",
    "\n",
    "X = cv_dm.toarray() \n",
    "print(type(X))\n",
    "\n",
    "y = jobdf['Title'].values #this is an array of labels\n",
    "print(type(y))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) #random_state is set seed\n",
    "\n",
    "# function creates 4 output structures - order matters\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=42, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "0.86511627907\n",
      "accuracy: 0.86511627907\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "        accountant       0.87      0.91      0.89        57\n",
      "  chief accountant       0.82      0.79      0.81        34\n",
      "    java developer       0.90      0.53      0.67        17\n",
      "            lawyer       1.00      1.00      1.00        28\n",
      "    office manager       1.00      0.93      0.97        15\n",
      "   project manager       0.95      0.90      0.93        21\n",
      "software developer       0.74      0.92      0.82        25\n",
      "     web developer       0.74      0.78      0.76        18\n",
      "\n",
      "       avg / total       0.87      0.87      0.86       215\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf9_nm = \"Logistic Regression 80/20, dict, new stopwords, binary\"\n",
    "\n",
    "# fit a logistic regression model to the data\n",
    "model = LogisticRegression(random_state = 42)\n",
    "print(model)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "clf9_expected = y_test\n",
    "clf9_predicted = model.predict(X_test)\n",
    "\n",
    "print(model.score(X_test, y_test))\n",
    "\n",
    "# summarize the fit of the model\n",
    "print(\"accuracy: \" + str(metrics.accuracy_score(clf9_expected, clf9_predicted)))\n",
    "print(metrics.classification_report(clf9_expected, clf9_predicted))\n",
    "\n",
    "clf9_accuracy = metrics.accuracy_score(clf9_expected, clf9_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which model was the best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Decision Tree (initial)</td>\n",
       "      <td>0.789474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression initial</td>\n",
       "      <td>0.839009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>K Nearest Neighbors initial (k=3)</td>\n",
       "      <td>0.718266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>K Nearest Neighbors (k=1 optimal)</td>\n",
       "      <td>0.733746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Logistic Regression binary CV</td>\n",
       "      <td>0.851393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Decision Tree binary CV</td>\n",
       "      <td>0.743034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Logistic Regression 80/20, binary = True</td>\n",
       "      <td>0.869767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Logistic Regression 80/20, binary, Stem</td>\n",
       "      <td>0.832558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Logistic Regression 80/20, Stem, stopwords</td>\n",
       "      <td>0.818605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Logistic Regression 80/20, tfidf</td>\n",
       "      <td>0.818605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Logistic Regression 80/20, dict, new stopwords, binary</td>\n",
       "      <td>0.865116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     Model  Accuracy\n",
       "0                                  Decision Tree (initial)  0.789474\n",
       "1                              Logistic Regression initial  0.839009\n",
       "2                        K Nearest Neighbors initial (k=3)  0.718266\n",
       "3                        K Nearest Neighbors (k=1 optimal)  0.733746\n",
       "4                            Logistic Regression binary CV  0.851393\n",
       "5                                  Decision Tree binary CV  0.743034\n",
       "6                 Logistic Regression 80/20, binary = True  0.869767\n",
       "7                  Logistic Regression 80/20, binary, Stem  0.832558\n",
       "8               Logistic Regression 80/20, Stem, stopwords  0.818605\n",
       "9                         Logistic Regression 80/20, tfidf  0.818605\n",
       "10  Logistic Regression 80/20, dict, new stopwords, binary  0.865116"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = [clf1_nm, clf2_nm, knn1_nm, knn2_nm, clf3_nm, clf4_nm, clf5_nm, clf6_nm, clf7_nm, clf8_nm, clf9_nm]\n",
    "model_accuracy = [clf1_accuracy, clf2_accuracy, knn1_accuracy, knn2_accuracy, clf3_accuracy, clf4_accuracy, clf5_accuracy, clf6_accuracy, clf7_accuracy, clf8_accuracy, clf9_accuracy]\n",
    "\n",
    "summarydf = pd.DataFrame(model_name, columns=['Model'])\n",
    "summarydf['Accuracy']=(model_accuracy)\n",
    "summarydf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 - K Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#how many clusters? this takes some time\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# finding an optimal value for k\n",
    "k_range = range(1,15)\n",
    "k_means_set = [KMeans(n_clusters=k,init='k-means++', max_iter=100, random_state = 42).fit(job_dm) for k in k_range]\n",
    "centroids_list = [km_result.cluster_centers_ for km_result in k_means_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40736.850093109322, 38842.336625729484, 38142.729841131419, 37800.623629931069, 36988.295668000515, 36802.010100074294, 36271.186586563374, 36211.865802503613, 35849.570937094904, 35455.699938821184, 35274.209293426153, 34960.591948314512, 34598.867028532695, 34605.614455878756]\n",
      "[  5.45696821e-10   1.89451347e+03   2.59412025e+03   2.93622646e+03\n",
      "   3.74855443e+03   3.93483999e+03   4.46566351e+03   4.52498429e+03\n",
      "   4.88727916e+03   5.28115015e+03   5.46264080e+03   5.77625814e+03\n",
      "   6.13798306e+03   6.13123564e+03]\n"
     ]
    }
   ],
   "source": [
    "# calc euclidean dist from each point to each cluster center\n",
    "from scipy.spatial.distance import cdist, pdist\n",
    "\n",
    "k_euclid = [cdist(job_dm, thing, 'euclidean') for thing in centroids_list]\n",
    "distance_set = [np.min(k_euc, axis=1) for k_euc in k_euclid]\n",
    "\n",
    "# total within-cluster sum of squares\n",
    "wcss = [np.sum(distance**2) for distance in distance_set]\n",
    "\n",
    "# total sum of squares\n",
    "tss  = np.sum(pdist(job_dm)**2) / job_dm.shape[0]\n",
    "\n",
    "# between cluster sum of squares\n",
    "bss = tss - wcss\n",
    "\n",
    "print(wcss)\n",
    "print(bss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEXCAYAAAC3c9OwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ndlsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x16a6fafbcc0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plot elbow chart\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(k_range, bss/tss*100, '^-')\n",
    "ax.set_ylim((0,100))\n",
    "plt.grid(True)\n",
    "plt.xlabel('K n_clusters')\n",
    "plt.ylabel('% Variance Explained')\n",
    "plt.title('% Var Explained vs K')\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization complete\n",
      "Iteration  0, inertia 1806.606\n",
      "Iteration  1, inertia 967.466\n",
      "Iteration  2, inertia 958.202\n",
      "Iteration  3, inertia 953.876\n",
      "Iteration  4, inertia 951.672\n",
      "Iteration  5, inertia 949.839\n",
      "Iteration  6, inertia 948.230\n",
      "Iteration  7, inertia 946.728\n",
      "Iteration  8, inertia 945.462\n",
      "Iteration  9, inertia 944.931\n",
      "Iteration 10, inertia 944.741\n",
      "Iteration 11, inertia 944.719\n",
      "Iteration 12, inertia 944.710\n",
      "Converged at iteration 12: center shift 0.000000e+00 within tolerance 2.670418e-08\n",
      "4    266\n",
      "2    196\n",
      "0    185\n",
      "5    139\n",
      "3    112\n",
      "7     91\n",
      "6     46\n",
      "1     39\n",
      "Name: clusters, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words=my_stopwords, max_df=.9, binary = True)\n",
    "tfidf_dm = tfidf.fit_transform(jobdf['JobRequirment'])\n",
    "\n",
    "My_k = 8\n",
    "km = KMeans(n_clusters=My_k, init='k-means++', max_iter=100, n_init=1, random_state = 42, verbose=True)\n",
    "km.fit(tfidf_dm)\n",
    "\n",
    "clusters = km.labels_.tolist()\n",
    "\n",
    "jobdf['clusters'] = clusters\n",
    "print(jobdf['clusters'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clusters</th>\n",
       "      <th>Title</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Accountant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Project Manager</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Office Manager</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Lawyer</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Accountant</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>Chief Accountant</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>Project Manager</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>Office Manager</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>Accountant</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>Chief Accountant</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>Office Manager</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>Chief Accountant</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>Accountant</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4</td>\n",
       "      <td>Chief Accountant</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4</td>\n",
       "      <td>Lawyer</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>Accountant</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4</td>\n",
       "      <td>Office Manager</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4</td>\n",
       "      <td>Java Developer</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>Software Developer</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4</td>\n",
       "      <td>Web Developer</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4</td>\n",
       "      <td>Project Manager</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>5</td>\n",
       "      <td>Chief Accountant</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5</td>\n",
       "      <td>Accountant</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>6</td>\n",
       "      <td>Chief Accountant</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>6</td>\n",
       "      <td>Accountant</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>7</td>\n",
       "      <td>Web Developer</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>7</td>\n",
       "      <td>Java Developer</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>7</td>\n",
       "      <td>Software Developer</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    clusters               Title  counts\n",
       "0          0          Accountant       1\n",
       "3          0     Project Manager       2\n",
       "2          0      Office Manager      41\n",
       "1          0              Lawyer     141\n",
       "4          1          Accountant      11\n",
       "5          1    Chief Accountant      28\n",
       "9          2     Project Manager      11\n",
       "8          2      Office Manager      35\n",
       "6          2          Accountant      74\n",
       "7          2    Chief Accountant      76\n",
       "12         3      Office Manager       1\n",
       "11         3    Chief Accountant      31\n",
       "10         3          Accountant      80\n",
       "14         4    Chief Accountant       2\n",
       "16         4              Lawyer       3\n",
       "13         4          Accountant       4\n",
       "17         4      Office Manager       4\n",
       "15         4      Java Developer      42\n",
       "19         4  Software Developer      55\n",
       "20         4       Web Developer      60\n",
       "18         4     Project Manager      96\n",
       "22         5    Chief Accountant      48\n",
       "21         5          Accountant      91\n",
       "24         6    Chief Accountant      15\n",
       "23         6          Accountant      31\n",
       "27         7       Web Developer      24\n",
       "25         7      Java Developer      31\n",
       "26         7  Software Developer      36"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clust_8 = jobdf.groupby(['clusters', 'Title']).size().reset_index(name='counts').sort_values(['clusters','counts'])\n",
    "\n",
    "clust_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
